---
title: "Update Twitter MEP"
author: "Giorgio Comai (OBCT/CCI)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
if (!require("pacman")) install.packages("pacman",  repos = "https://cloud.r-project.org") # for taking care of package installation/loading
pacman::p_load("tidyverse")
pacman::p_load("ROAuth")
pacman::p_load("rtweet")
pacman::p_load("stringr")
# pacman::p_load("devtools")
# devtools::install_github("mkearney/rtweet")
# library("rtweet")

dir.create(path = file.path("archive"), showWarnings = FALSE)
dir.create(path = file.path("DF"), showWarnings = FALSE)
dir.create(path = file.path("logs"), showWarnings = FALSE)

twitter_token <- readRDS(file = "twitter_token.rds")
```



```{r updateTweets, eval=TRUE, message=FALSE, warning=FALSE}
# get list of all users
fileInfo <- file.info(list.files(path = file.path("DF"), full.names = TRUE))

users <- stringr::str_replace_all(string = list.files(path = file.path("DF")),
                                  pattern = stringr::fixed(".rds"), 
                                  replacement = "")
newTweets <- tibble::data_frame(users = users, newTweets = NA)

for (i in order(fileInfo$mtime)) {  # start processing by oldest modified
  stored <- readRDS(file = rownames(fileInfo)[i])
  if (is.null(stored)==FALSE) { # 
    # if there's an error, print it but go ahead
    temp <- tryCatch(expr = get_timeline(user = users[i], n = 3200, min_id = max(stored$status_id), token = twitter_token),
                     error = function(e) {
                       # do nothing
                     })
    # if Twitter throws back anything looking real, add it to stored file
    if (is.null(temp)==FALSE) {
      if (nrow(temp)>0) {
        if (is.na(temp$screen_name[1])==FALSE){
          preSave <- bind_rows(temp, stored) %>% distinct(status_id, .keep_all = TRUE) %>% arrange(created_at)
          saveRDS(object = preSave, file = file.path("DF", paste0(users[i], ".rds")))
          # store how many new tweets in data frame for reference
          newTweets$newTweets[i] <- nrow(preSave)-nrow(stored)
          message(paste(newTweets[i,], collapse = " - "))
          Sys.sleep(time = 1)
        }
      }
    }
  }
}
knitr::kable(x = newTweets %>% arrange(newTweets))
```

# Pre-process data

Include only data for the last 91 days, and remove retweets. 

```{r}
# transform into data frame

allMEPtweetsDF <- setNames(data.frame(matrix(ncol = 9, nrow = 0)), c("screen_name", "date", "text", "status_id", "retweet_count", "favorite_count", "lang", "hashtags", "urls_expanded"))

for (i in list.files(path = file.path("DF"), full.names = TRUE)) {
  temp <- readRDS(file = i) 
  if (is.null(temp)==FALSE) {
    allMEPtweetsDF <- bind_rows(allMEPtweetsDF, temp %>%
                                  distinct(status_id, .keep_all = TRUE) %>%
                                  mutate(date = as.Date(created_at)) %>% 
                                  rename(time = created_at) %>% 
                                  filter(date>(Sys.Date()-91)) %>% 
                                  filter(is_retweet==FALSE)) %>% 
      select(screen_name, date, time, text, status_id, retweet_count, favorite_count, lang, hashtags, urls_expanded)
  }
}

## clean text

allMEPtweetsDF$clean_text <- stringr::str_replace_all(
  string = allMEPtweetsDF$text,
  pattern = stringr::regex(pattern = "@[[:alnum:]]+|#[[:alnum:]]+|http[[:graph:]]+|RT ", ignore_case = FALSE), replacement = "")

allMEPtweetsDF$clean_text <- stringr::str_replace_all(string = allMEPtweetsDF$clean_text, pattern = stringr::fixed("&amp;"), replacement = "&") 

allMEPtweetsDF$Link <- paste0("<a href='https://twitter.com/", allMEPtweetsDF$screen_name, "/status/", allMEPtweetsDF$status_id, "'  target='_blank'>Source</a>")

```

```{r metadata on MEPs}

dir.create(path = file.path("data"), showWarnings = FALSE)

## get list of twitter accounts of all MEPs
	if (file.exists(file.path("data", "MEPsDF.rds"))==FALSE) {
	  MEPsDF <- read_csv(file = "https://raw.githubusercontent.com/eliflab/European-Parliament-Open-Data/master/meps_full_list_with_twitter_accounts.csv") %>%
	    mutate(SCREEN_NAME = stringr::str_replace_all(string = SCREEN_NAME, pattern = stringr::fixed("@"), replacement = ""))
	  saveRDS(object = MEPsDF, file = file.path("data", "MEPsDF.rds"))
	} else {
	  MEPsDF <- read_rds(file.path("data", "MEPsDF.rds"))
	}

### Short EP group names
allMEPtweetsDF <- left_join(allMEPtweetsDF, MEPsDF %>% rename(screen_name = SCREEN_NAME), by = "screen_name")

groupShortName <- data_frame(Long = unique(MEPsDF$GROUP)[is.na(unique(MEPsDF$GROUP))==FALSE], Short = c("EPP", "EFDD", "Greensâ€“EFA", "S&D", "GUE-NGL", "ENF", "ALDE", "ECR", "NI"))

allMEPtweetsDF$GroupShort <- allMEPtweetsDF$GROUP

for (i in seq_along(along.with = groupShortName$Long)) {
  allMEPtweetsDF$GroupShort <- stringr::str_replace_all(string = allMEPtweetsDF$GroupShort, pattern = stringr::fixed(groupShortName$Long[i]), replacement = groupShortName$Short[i])
}


```

Feed datasets to QuoteFinder


```{r}
dataFolder <- "/srv/shiny-server/QuoteFinderApp/data"
dir.create(dataFolder, showWarnings = FALSE)

saveRDS(object = allMEPtweetsDF, file = file.path(dataFolder, "dataset.rds"))  

lang <- data_frame(lang = unlist(allMEPtweetsDF$lang)) %>%
  tidyr::drop_na() %>%
  count(lang, sort = TRUE) %>% select(lang)
langL <- as.list(lang$lang)

saveRDS(object = langL, file = file.path(dataFolder, "lang.rds"))   

hashtags <- vector("list", length = length(langL))
hashtags <- setNames(object = hashtags, nm = unlist(langL))
for (i in seq_along(langL)) {
  tempL <- data_frame(hashtags = allMEPtweetsDF %>%
               filter(lang==langL[[i]]) %>%
               select(hashtags) %>% 
               unlist()) %>% 
    tidyr::drop_na()  %>%
    count(hashtags, sort = TRUE) %>% # make hashtags in order of most frequent, by language
    mutate(hashtagsLower = tolower(hashtags)) %>% # ignore case, but keep the case of the most frequently found case combination
    group_by(hashtagsLower) %>%
    add_tally(wt = n) %>%
    distinct(hashtagsLower, .keep_all = TRUE) %>%
    arrange(desc(nn)) %>% 
    ungroup() %>%
    select(hashtags) %>% 
    pull(hashtags) %>%
    as.list()
  if (length(tempL) == 0) {
     names(tempL) <- NULL
  } else {
    names(tempL) <- paste0("#", unlist(tempL))
  }
  hashtags[[i]] <- tempL
}

saveRDS(object = hashtags, file = file.path(dataFolder, "hashtags.rds"))

countries <- as.list(unique(MEPsDF$NATIONALITY))

saveRDS(object = countries, file = file.path(dataFolder, "countries.rds"))

EPGroupShort <- as.list(unique(allMEPtweetsDF$GroupShort))

saveRDS(object = EPGroupShort, file = file.path(dataFolder, "EPGroupShort.rds"))


if (file.exists(file.path(dataFolder, "langCode.rds"))==FALSE) {
  langCode <- read_csv("https://pkgstore.datahub.io/core/language-codes/language-codes_csv/data/b65af208b52970a4683fa8fde9af8e9f/language-codes_csv.csv") %>% add_case(alpha2 = "und", English = "Undetermined")
  
  saveRDS(object = langCode, file = file.path(dataFolder, "langCode.rds"))
}


```

